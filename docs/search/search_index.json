{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Labanotation Suite Authors: David Baumert, Zhaoyuan Ma, John Lee, Sven Pleyer, Takuya Kiyokawa, Katsushi Ikeuchi This Labanotation Suite repository contains a robot gesture authoring system comprised of software tools, source code, simulation software and sample data that supports experimentation with the concepts presented in the paper Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots (International Journal of Computer Vision, December 2018) . This suite can be used to help robots gesture in natural and meaningful ways. Contributing This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Robot Gesture Authoring System Description System Modules: System Diagram: Gesture Authoring Tools KinectReader A Windows application that connects to a Kinect sensor device and provides a user interface for capturing and storing gestures performed by human subjects. It's primary output data is human stick-figure joint positions in a .csv format, but can also capture corresponding RGB video and audio at the same time. KinectCaptureEditor A Windows application that loads human joint position .csv files produced by the KinectReader or other Gesture Authoring Tools, as well as optional corresponding video and audio files. It provides a timeline-based method to edit audio and joint movement sequences into meaningful gestures. LabanEditor A Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams used in the interface. MSRAbot Simulation Software A ready-to-run file tree that can be added to an HTTP server and viewed by most modern web browsers. The files implement a javascript simulation environment that hosts an animated model of the MSRAbot humanoid robot, along with viewing controls and an ability to render new JSON gesture files created using the Gesture Authoring Tools in this repository. FAQ Q Who should I contact regarding this repository? A Please create a Github issue or email robotics@microsoft.com with any questions or feedback. Q Is this code suitable for non-robotic applications such as documenting dance steps? A Currently, the gesture capture system only addresses movement of the upper human torso. However, we may expand to capturing the entire human skeleton in the future. Citation If you want to cite this work, please use the following bibtex code @Article{Ikeuchi2018, author= Ikeuchi, Katsushi and Ma, Zhaoyuan and Yan, Zengqiang and Kudoh, Shunsuke and Nakamura, Minako , title= Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots , journal= International Journal of Computer Vision , year= 2018 , month= Dec , day= 01 , volume= 126 , number= 12 , pages= 1415--1429 , issn= 1573-1405 , doi= 10.1007/s11263-018-1123-1 , url= https://doi.org/10.1007/s11263-018-1123-1 }","title":"Home"},{"location":"#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"#labanotation-suite","text":"Authors: David Baumert, Zhaoyuan Ma, John Lee, Sven Pleyer, Takuya Kiyokawa, Katsushi Ikeuchi This Labanotation Suite repository contains a robot gesture authoring system comprised of software tools, source code, simulation software and sample data that supports experimentation with the concepts presented in the paper Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots (International Journal of Computer Vision, December 2018) . This suite can be used to help robots gesture in natural and meaningful ways.","title":"Labanotation Suite"},{"location":"#contributing","text":"This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contributing"},{"location":"#robot-gesture-authoring-system-description","text":"","title":"Robot Gesture Authoring System Description"},{"location":"#system-modules","text":"","title":"System Modules:"},{"location":"#system-diagram","text":"","title":"System Diagram:"},{"location":"#gesture-authoring-tools","text":"","title":"Gesture Authoring Tools"},{"location":"#kinectreader","text":"A Windows application that connects to a Kinect sensor device and provides a user interface for capturing and storing gestures performed by human subjects. It's primary output data is human stick-figure joint positions in a .csv format, but can also capture corresponding RGB video and audio at the same time.","title":"KinectReader"},{"location":"#kinectcaptureeditor","text":"A Windows application that loads human joint position .csv files produced by the KinectReader or other Gesture Authoring Tools, as well as optional corresponding video and audio files. It provides a timeline-based method to edit audio and joint movement sequences into meaningful gestures.","title":"KinectCaptureEditor"},{"location":"#labaneditor","text":"A Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams used in the interface.","title":"LabanEditor"},{"location":"#msrabot-simulation-software","text":"A ready-to-run file tree that can be added to an HTTP server and viewed by most modern web browsers. The files implement a javascript simulation environment that hosts an animated model of the MSRAbot humanoid robot, along with viewing controls and an ability to render new JSON gesture files created using the Gesture Authoring Tools in this repository.","title":"MSRAbot Simulation Software"},{"location":"#faq","text":"Q Who should I contact regarding this repository? A Please create a Github issue or email robotics@microsoft.com with any questions or feedback. Q Is this code suitable for non-robotic applications such as documenting dance steps? A Currently, the gesture capture system only addresses movement of the upper human torso. However, we may expand to capturing the entire human skeleton in the future.","title":"FAQ"},{"location":"#citation","text":"If you want to cite this work, please use the following bibtex code @Article{Ikeuchi2018, author= Ikeuchi, Katsushi and Ma, Zhaoyuan and Yan, Zengqiang and Kudoh, Shunsuke and Nakamura, Minako , title= Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots , journal= International Journal of Computer Vision , year= 2018 , month= Dec , day= 01 , volume= 126 , number= 12 , pages= 1415--1429 , issn= 1573-1405 , doi= 10.1007/s11263-018-1123-1 , url= https://doi.org/10.1007/s11263-018-1123-1 }","title":"Citation"},{"location":"CODE_OF_CONDUCT/","text":"Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"CODE_OF_CONDUCT/#microsoft-open-source-code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"SECURITY/","text":"Security Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's Microsoft's definition of a security vulnerability of a security vulnerability, please report it to us as described below. Reporting Security Issues Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs. Preferred Languages We prefer all communications to be in English. Policy Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"SECURITY"},{"location":"SECURITY/#security","text":"Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's Microsoft's definition of a security vulnerability of a security vulnerability, please report it to us as described below.","title":"Security"},{"location":"SECURITY/#reporting-security-issues","text":"Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs.","title":"Reporting Security Issues"},{"location":"SECURITY/#preferred-languages","text":"We prefer all communications to be in English.","title":"Preferred Languages"},{"location":"SECURITY/#policy","text":"Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"Policy"},{"location":"GestureAuthoringTools/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Labanotation Suite Gesture Authoring Tools KinectReader A compiled Windows application that connects to a Kinect sensor device and provides a user interface for capturing and storing gestures performed by human subjects. It's primary output data is human stick-figure joint positions in a .csv format, but can also capture corresponding RGB video and audio at the same time. KinectCaptureEditor A compiled Windows application that loads human joint position .csv files produced by the KinectReader or other tools, as well as optional corresponding video and audio files. It provides a timeline-based method to trim audio and video joint movement sequences into representative human gestures. LabanEditor A Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams used in the interface.","title":"Home"},{"location":"GestureAuthoringTools/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"GestureAuthoringTools/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"GestureAuthoringTools/#labanotation-suite","text":"","title":"Labanotation Suite"},{"location":"GestureAuthoringTools/#gesture-authoring-tools","text":"","title":"Gesture Authoring Tools"},{"location":"GestureAuthoringTools/#kinectreader","text":"A compiled Windows application that connects to a Kinect sensor device and provides a user interface for capturing and storing gestures performed by human subjects. It's primary output data is human stick-figure joint positions in a .csv format, but can also capture corresponding RGB video and audio at the same time.","title":"KinectReader"},{"location":"GestureAuthoringTools/#kinectcaptureeditor","text":"A compiled Windows application that loads human joint position .csv files produced by the KinectReader or other tools, as well as optional corresponding video and audio files. It provides a timeline-based method to trim audio and video joint movement sequences into representative human gestures.","title":"KinectCaptureEditor"},{"location":"GestureAuthoringTools/#labaneditor","text":"A Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams used in the interface.","title":"LabanEditor"},{"location":"GestureAuthoringTools/KinectCaptureEditor/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics KinectCaptureEditor User Manual Introduction The KinectCaptureEditor tool is provided as a Windows binary application that displays skeletal tracking data and/or audio/video captured by the KinectReader tool. Sections of the captured sessions may be trimmed and saved to create new data files of concise gestures. Tested System Software Microsoft Windows 10, 64-bit Microsoft Visual C++ Runtime Library (https://aka.ms/vs/16/release/vc_redist.x64.exe) Installation Download the KinectCaptureEditor.exe file to a folder on your local machine. Example Use The user wishes to create a set of reference gestures with associated audio and video. Each individual gesture is to have its own Kinect data and media file. - Using the KinectReader tool, the user captures themselves making the series of poses and vocally naming each pose. - The captured session is loaded into the KinectCaptureEditor using File\u2192Load Kinect Data\u2026 and File\u2192Load Audio/Video File\u2026 or Ctrl+O and Shift+Ctrl+O keys. - The range cursors are positioned to isolate the Kinect data, video, and audio excerpts for the first gesture. - The range is played via File\u2192Play Range or F5 key to verify the excerpt. - The Kinect data and media files for the first gesture are created using File\u2192Write Kinect Data\u2026 and File\u2192Write Audio/Video File\u2026 or Ctrl+S and Shift+Ctrl+S keys. - The range cursors are positioned for the next gesture and the procedure repeated until all individual gesture files have been created. Starting Up and Loading Files To begin, double-click KinectCaptureEditor.exe to run the application. Load the Kinect data by selecting File\u2192Load Kinect Data\u2026 from the menu bar or pressing Ctrl+S to display the open file dialog. Navigate to the folder containing the Kinect data created by the KinectReader tool, select the file (usually named \u201cjoints.csv\u201d ) and click Open . Load the corresponding audio/video file by selecting File\u2192Load Audio/Video File\u2026 or pressing Shift+Ctrl+S to display the open file dialog and select the media file corresponding to the Kinect data (usually named \u201caudiorecording.wav\u201d or \u201cvideorecording.wmv\u201d .) Note Loading both skeletal tracking data and audio/video files is not required. If both are loaded, the Kinect data and media files need not be the same length. Media files may be any format supported by Windows Media Foundation. The User Interface The main application window is divided into three panes. Drag the gray dividers between the panes to size the panes as desired. Timeline Pane The Timeline Pane at the bottom shows the individual Kinect data frames (snapshots of the Kinect skeletal tracking data) and audio waveform in chronological order. Note that the Kinect data and media files always start at 0.0 seconds. Even if they are different durations, the starting time of one or the other currently cannot be changed. - The horizontal scale indicates the time in seconds. - Each tick at the top indicates a Kinect data frame and a slightly longer, brighter tick indicates every tenth data frame. As space permits, a data frame tick extends to a thumbnail of that frame. - If a media file is loaded and contains audio, the Audio Waveform displays a graphical representation. - The yellow, red, and green vertical lines with diamond-shaped handles are cursors, described below in Timeline Cursors . Timeline Cursors Cursors are the vertical lines with diamond-shaped handles in the Timeline Pane. The three cursors are: - Current Position (yellow)\u2014This cursor indicates the data and video frames shown in the Frame Display Pane and the current position during playback (see Playback, below.) This cursor is useful for exploring the data without changing the currently selected range. - Range Start (green)\u2014The selected range starts at this cursor. The selected range is used for playback and creating excerpts. This cursor can\u2019t be dragged beyond the Range End cursor. - Range End (red)\u2014The selected range ends at and includes the frames at this cursor. Dragging this cursor beyond the Range Start cursor moves the Range Start cursor as well. The position of the Current Position Cursor and the selected range is shown in the Summary Pane. To move a cursor, left click, drag, and release the diamond handle at the bottom. The Frame Display Pane will show the frame at the cursor\u2019s position and audio (if loaded) will play while the cursor is being dragged. Frame Display Pane The Frame Display Pane at the upper-left displays a diagram of the Kinect data frame and corresponding video frame if a media file is loaded. - The Data Frame Number is the index of the Kinect data frame where 1 is the first. - The Relative Time is the time (from the start of the data and media files) of the Kinect Data Frame and Video Frame shown. - The Kinect Sensor Timestamp is the value assigned to the frame by the Kinect sensor when it was captured. - The Kinect Data Diagram is the graphical representation of the frame\u2019s Kinect data. - The Video Frame is the video at the Relative Time if the loaded media file contains video. - The pane usually shows the frame at the Current Position cursor (yellow) but when any cursor is clicked on or dragged, the pane shows the frame for that cursor\u2019s position. When the cursor is released, the pane returns to displaying frame for the Current Position cursor. - Note: When a cursor is dragged, the pane must load the new video frame(s) from the media file and the video frame display may update slowly and lag the cursor. Summary Pane The Summary Pane at the upper-right displays a summary of the current session. - The Kinect Data File is the currently loaded file, the number of data frames, and the total data frame series duration in seconds. - The Media File is the currently loaded file and the total media duration in seconds. - The Selected Range is starting and ending time range in seconds and Kinect data frames as marked by the Range Start (green) and Range End (red) cursors. - The Current Position is the time position of the Current Position cursor (yellow) relative to the start of the Kinect data and media files. Playback There are two ways to play the currently loaded Kinect data and audio/video: - Selecting Play\u2192Play Range from the menu bar or pressing the F5 key will play the data frames and media within the selected range. - Selecting Play\u2192Play All or pressing Shift+F5 will play the entire timeline. KinectCaptureEditor plays the Kinect data, video, and audio as close to real-time as possible. The Current Position Cursor indicates the current time during playback and returns to its previous position when playback stops. Playback stops automatically at the end of the range or timeline and can be manually stopped by selecting Play\u2192Stop Play or pressing the Esc key. If the Kinect data or video track is shorter than the play duration, the Frame Display pane will show the last Kinect data frame and/or video frame. If the audio track is shorter, audio stops for the remainder of the play duration. Creating Excerpts To create an excerpt or portion of the loaded Kinect data frames and/or media file: 1. Set the desired range using the Range Start and Range End cursors. 1. To create a Kinect data excerpt, select File\u2192Write Kinect Data\u2026 or press Ctrl+S to display the save file dialog. Enter a new name for the Kinect data frames and click Save. 1. To create a media excerpt, select File\u2192Write Audio/Video File\u2026 or press Shift+Ctrl+S to display the save file dialog. Enter a new name for the media file and click Save. Excerpt files can be loaded back into KinectCaptureEditor using the File\u2192Load commands. The Range Start and Range End cursors need not be positioned exactly on the starting time of a Kinect data frame. KinectCaptureEditor will set to duration of the first and last data frames written to the file to match the cursor positions. Video and audio excerpts are adjusted similarly. Note: Media excerpts are always written using the Windows Media Audio 8/Video 8 codecs at this time. Reference Menus File Menu Menu Item Description New Clears currently loaded Kinect data and media tracks Load Kinect Data Loads Kinect data from a file Load Audio/Video File Loads a media file Write Kinect Data Writes the Kinect data in the selected range to a file Write Audio/Video File Writes the media tracks in the selected range to a file Play Menu Menu Item Description Play Range Play the selected range indicated by the Range Start and Range End cursors Play All Play the entire timeline Stop Play Stop playback Help Menu |Menu Item|Description| |--|--| |About\u2026|Display the About dialog box| Keyboard Shortcuts Keys Description Ctrl+O Open a Kinect data file Shift+Ctrl+O Open a media file F5 Play the selected range Shift+F5 Play the entire timeline Esc Stop playing Ctrl+S Write range to a Kinect data file Shift+Ctrl+S Write range to a media file Alt+F4 Exit the application","title":"Home"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#kinectcaptureeditor-user-manual","text":"","title":"KinectCaptureEditor User Manual"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#introduction","text":"The KinectCaptureEditor tool is provided as a Windows binary application that displays skeletal tracking data and/or audio/video captured by the KinectReader tool. Sections of the captured sessions may be trimmed and saved to create new data files of concise gestures.","title":"Introduction"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#tested-system-software","text":"Microsoft Windows 10, 64-bit Microsoft Visual C++ Runtime Library (https://aka.ms/vs/16/release/vc_redist.x64.exe)","title":"Tested System Software"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#installation","text":"Download the KinectCaptureEditor.exe file to a folder on your local machine.","title":"Installation"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#example-use","text":"The user wishes to create a set of reference gestures with associated audio and video. Each individual gesture is to have its own Kinect data and media file. - Using the KinectReader tool, the user captures themselves making the series of poses and vocally naming each pose. - The captured session is loaded into the KinectCaptureEditor using File\u2192Load Kinect Data\u2026 and File\u2192Load Audio/Video File\u2026 or Ctrl+O and Shift+Ctrl+O keys. - The range cursors are positioned to isolate the Kinect data, video, and audio excerpts for the first gesture. - The range is played via File\u2192Play Range or F5 key to verify the excerpt. - The Kinect data and media files for the first gesture are created using File\u2192Write Kinect Data\u2026 and File\u2192Write Audio/Video File\u2026 or Ctrl+S and Shift+Ctrl+S keys. - The range cursors are positioned for the next gesture and the procedure repeated until all individual gesture files have been created.","title":"Example Use"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#starting-up-and-loading-files","text":"To begin, double-click KinectCaptureEditor.exe to run the application. Load the Kinect data by selecting File\u2192Load Kinect Data\u2026 from the menu bar or pressing Ctrl+S to display the open file dialog. Navigate to the folder containing the Kinect data created by the KinectReader tool, select the file (usually named \u201cjoints.csv\u201d ) and click Open . Load the corresponding audio/video file by selecting File\u2192Load Audio/Video File\u2026 or pressing Shift+Ctrl+S to display the open file dialog and select the media file corresponding to the Kinect data (usually named \u201caudiorecording.wav\u201d or \u201cvideorecording.wmv\u201d .) Note Loading both skeletal tracking data and audio/video files is not required. If both are loaded, the Kinect data and media files need not be the same length. Media files may be any format supported by Windows Media Foundation.","title":"Starting Up and Loading Files"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#the-user-interface","text":"The main application window is divided into three panes. Drag the gray dividers between the panes to size the panes as desired.","title":"The User Interface"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#timeline-pane","text":"The Timeline Pane at the bottom shows the individual Kinect data frames (snapshots of the Kinect skeletal tracking data) and audio waveform in chronological order. Note that the Kinect data and media files always start at 0.0 seconds. Even if they are different durations, the starting time of one or the other currently cannot be changed. - The horizontal scale indicates the time in seconds. - Each tick at the top indicates a Kinect data frame and a slightly longer, brighter tick indicates every tenth data frame. As space permits, a data frame tick extends to a thumbnail of that frame. - If a media file is loaded and contains audio, the Audio Waveform displays a graphical representation. - The yellow, red, and green vertical lines with diamond-shaped handles are cursors, described below in Timeline Cursors .","title":"Timeline Pane"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#timeline-cursors","text":"Cursors are the vertical lines with diamond-shaped handles in the Timeline Pane. The three cursors are: - Current Position (yellow)\u2014This cursor indicates the data and video frames shown in the Frame Display Pane and the current position during playback (see Playback, below.) This cursor is useful for exploring the data without changing the currently selected range. - Range Start (green)\u2014The selected range starts at this cursor. The selected range is used for playback and creating excerpts. This cursor can\u2019t be dragged beyond the Range End cursor. - Range End (red)\u2014The selected range ends at and includes the frames at this cursor. Dragging this cursor beyond the Range Start cursor moves the Range Start cursor as well. The position of the Current Position Cursor and the selected range is shown in the Summary Pane. To move a cursor, left click, drag, and release the diamond handle at the bottom. The Frame Display Pane will show the frame at the cursor\u2019s position and audio (if loaded) will play while the cursor is being dragged.","title":"Timeline Cursors"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#frame-display-pane","text":"The Frame Display Pane at the upper-left displays a diagram of the Kinect data frame and corresponding video frame if a media file is loaded. - The Data Frame Number is the index of the Kinect data frame where 1 is the first. - The Relative Time is the time (from the start of the data and media files) of the Kinect Data Frame and Video Frame shown. - The Kinect Sensor Timestamp is the value assigned to the frame by the Kinect sensor when it was captured. - The Kinect Data Diagram is the graphical representation of the frame\u2019s Kinect data. - The Video Frame is the video at the Relative Time if the loaded media file contains video. - The pane usually shows the frame at the Current Position cursor (yellow) but when any cursor is clicked on or dragged, the pane shows the frame for that cursor\u2019s position. When the cursor is released, the pane returns to displaying frame for the Current Position cursor. - Note: When a cursor is dragged, the pane must load the new video frame(s) from the media file and the video frame display may update slowly and lag the cursor.","title":"Frame Display Pane"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#summary-pane","text":"The Summary Pane at the upper-right displays a summary of the current session. - The Kinect Data File is the currently loaded file, the number of data frames, and the total data frame series duration in seconds. - The Media File is the currently loaded file and the total media duration in seconds. - The Selected Range is starting and ending time range in seconds and Kinect data frames as marked by the Range Start (green) and Range End (red) cursors. - The Current Position is the time position of the Current Position cursor (yellow) relative to the start of the Kinect data and media files.","title":"Summary Pane"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#playback","text":"There are two ways to play the currently loaded Kinect data and audio/video: - Selecting Play\u2192Play Range from the menu bar or pressing the F5 key will play the data frames and media within the selected range. - Selecting Play\u2192Play All or pressing Shift+F5 will play the entire timeline. KinectCaptureEditor plays the Kinect data, video, and audio as close to real-time as possible. The Current Position Cursor indicates the current time during playback and returns to its previous position when playback stops. Playback stops automatically at the end of the range or timeline and can be manually stopped by selecting Play\u2192Stop Play or pressing the Esc key. If the Kinect data or video track is shorter than the play duration, the Frame Display pane will show the last Kinect data frame and/or video frame. If the audio track is shorter, audio stops for the remainder of the play duration.","title":"Playback"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#creating-excerpts","text":"To create an excerpt or portion of the loaded Kinect data frames and/or media file: 1. Set the desired range using the Range Start and Range End cursors. 1. To create a Kinect data excerpt, select File\u2192Write Kinect Data\u2026 or press Ctrl+S to display the save file dialog. Enter a new name for the Kinect data frames and click Save. 1. To create a media excerpt, select File\u2192Write Audio/Video File\u2026 or press Shift+Ctrl+S to display the save file dialog. Enter a new name for the media file and click Save. Excerpt files can be loaded back into KinectCaptureEditor using the File\u2192Load commands. The Range Start and Range End cursors need not be positioned exactly on the starting time of a Kinect data frame. KinectCaptureEditor will set to duration of the first and last data frames written to the file to match the cursor positions. Video and audio excerpts are adjusted similarly. Note: Media excerpts are always written using the Windows Media Audio 8/Video 8 codecs at this time.","title":"Creating Excerpts"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#reference","text":"","title":"Reference"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#menus","text":"","title":"Menus"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#file-menu","text":"Menu Item Description New Clears currently loaded Kinect data and media tracks Load Kinect Data Loads Kinect data from a file Load Audio/Video File Loads a media file Write Kinect Data Writes the Kinect data in the selected range to a file Write Audio/Video File Writes the media tracks in the selected range to a file","title":"File Menu"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#play-menu","text":"Menu Item Description Play Range Play the selected range indicated by the Range Start and Range End cursors Play All Play the entire timeline Stop Play Stop playback","title":"Play Menu"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#help-menu","text":"|Menu Item|Description| |--|--| |About\u2026|Display the About dialog box|","title":"Help Menu"},{"location":"GestureAuthoringTools/KinectCaptureEditor/#keyboard-shortcuts","text":"Keys Description Ctrl+O Open a Kinect data file Shift+Ctrl+O Open a media file F5 Play the selected range Shift+F5 Play the entire timeline Esc Stop playing Ctrl+S Write range to a Kinect data file Shift+Ctrl+S Write range to a media file Alt+F4 Exit the application","title":"Keyboard Shortcuts"},{"location":"GestureAuthoringTools/KinectReader/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics KinectReader User Manual Introduction The KinectReader tool is provided as a Windows binary application that uses a Kinect sensor to capture upper-torso human gestures as body movement tracking sequences and optionally audio/video. Captured gestures can be edited using the KinectCaptureEditor application and are suitable for conversion to Labanotation using the LabanEditor application. Tested System Hardware and Software Microsoft Windows 10, 64-bit Microsoft Kinect Sensor v2 for Windows Microsoft Kinect SDK v2.0 or newer for Kinect Sensor v2 (https://www.microsoft.com/en-us/download/details.aspx?id=44561) OpenCV Version 3.4.3 for Windows (https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.3/opencv-3.4.3-vc14_vc15.exe/download) Microsoft Visual C++ Runtime Library (https://aka.ms/vs/16/release/vc_redist.x64.exe) Installation Download the entire KinectReader folder to your local machine - If needed, edit the file KinectReader.ini and set the DataDir property to a folder where the capture session data will be written. See INI File section below - Locate and place a copy of the OpenCV file opencv_world343.dll into the application folder, or add the existing location to your system's Path using Windows' Advanced System Properties control panel to set an Environment Variable . Example Use The user wishes to create a set of reference gestures with associated audio and video. - Launch the KinectReader app. - From the drop-down control on the right, select Record Audio Video. - Click on Start Capture. - Perform each gesture and vocally name the gesture. - Click on Stop Capture. - The Kinect body data frames are saved to joints.csv and the audio/video recording is saved to videorecording.wmv. Starting Up and Shutdown To run, double-click KinectReader.exe. To exit, click on the close button at the top of the window or press Alt+F4 and click Yes on the close confirmation message box. The User Interface The main application windows has three main areas: the Kinect camera display, status, and controls. Kinect Camera Display The Kinect Camera Display at the bottom displays the color image from the Kinect sensor overlaid with a diagram of the body tracking data for each person being tracked by the camera. Segments recognized by the Kinect sensor are green and inferred segments are blue. Segments that are not recognized are not shown. A red box highlights the person whose body tracking data will be recorded. If the wrong person is being tracked, click on the head of the correct person. Status The Status area at the top-left displays messages such as a capture session is active and when a frame is captured. Controls The Controls area at the top-right configure and control a recording session. - The media drop-down control specifies whether to record audio video, audio-only, or not at all along with the Kinect data. - The Start Capture button begins a continuous capture session. - The Start Mark button begins a frame-by-frame capture session. A Kinect data frame is only captured when the Mark button is pressed. - The Stop Capture button stop the capture session and records the Kinect data, audio, and video to the output files. The video is recorded from the Kinect\u2019s color camera and the audio is recorded from the system default audio input device. Set the input audio device or adjust the audio input level using the Windows Sound settings or control panel. Capture Sessions A continuous capture session records Kinect data frames, audio, and video as they are supplied by the Kinect sensor. A frame-by-frame capture session records a Kinect data frame only when the Mark button is clicked. When enabled, audio/video is recorded continuously for either session type. The session files are saved to the directory specified by the DataDir property: - The Kinect data frames are saved to joints.csv. - When recording both audio and video, the recording is saved to videorecording.wmv. - When recording audio only, the recording is saved to audiorecording.wma. Reference Keyboard Shortcuts Keys Description Alt+F4 Exit the application ## INI File [Section] Property=Value Type -- -- [main] DataDir= String ## Kinect Body Data File Format The Kinect body data file is a Comma-Separated Values (CSV) text file (*.csv). A data frame is a line of text terminated by a CR/LF pair and consists of a series of fields separated by commas (\u201c,\u201d). Each field is the C programming language printf()-style text representation of the value. The fields are the following: Field Type Description Timestamp Integer Kinect sensor timestamp in hundred-nanosecond units (10,000,000 units = 1 second.) The absolute value of the timestamp has no meaning but the timestamp allows calculating the elapsed time between frames and ordering frames by when they were captured. Joint Array Tracked Joint A series of Tracked Joint entries, one per each of 25 joints. The fields that make up the Joint Array immediately follow the Timestamp field. Each set of four fields form a Tracked Joint entry, one per each of 25 joints. There is no special delineation between entries in the array nor between each field in each entry beyond the standard comma. The Tracked Joint fields are: Field Type Description X Float X coordinate of the joint in Kinect camera space Y Float Y coordinate of the joint in Kinect camera space Z Float Z coordinate of the joint in Kinect camera space Tracking State Integer 0 = joint is not tracked, 1 = joint position is inferred (estimated), 2 = joint is tracked Kinect camera space is defined as follows: - The origin (X = 0, Y = 0, Z = 0) is at the center of the Kinect\u2019s IR sensor - X increases to the sensor\u2019s left, from the sensor looking at the joint - Y increases towards the sensor\u2019s top - Z increases away from the sensor to the joint - Each whole unit is one meter (1.0 units = 1.0 meters) See the Microsoft documentation Kinect for Windows SDK 2.0, \u201cProgramming Guide\u201d, \u201cCoordinate mapping\u201d for more details. The joints appear in the Joint Array in this order: 1. Spine Base 1. Spine Midsection 1. Neck 1. Head 1. Left Shoulder 1. Left Elbow 1. Left Wrist 1. Left Hand 1. Right Shoulder 1. Right Elbow 1. Right Wrist 1. Right Hand 1. Left Hip 1. Left Knee 1. Left Ankle 1. Left Foot 1. Right Hip 1. Right Knee 1. Right Ankle 1. Right Foot 1. Spine Shoulder 1. Left Hand Tip 1. Left Thumb 1. Right Hand Tip 1. Right Thumb","title":"Home"},{"location":"GestureAuthoringTools/KinectReader/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"GestureAuthoringTools/KinectReader/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"GestureAuthoringTools/KinectReader/#kinectreader-user-manual","text":"","title":"KinectReader User Manual"},{"location":"GestureAuthoringTools/KinectReader/#introduction","text":"The KinectReader tool is provided as a Windows binary application that uses a Kinect sensor to capture upper-torso human gestures as body movement tracking sequences and optionally audio/video. Captured gestures can be edited using the KinectCaptureEditor application and are suitable for conversion to Labanotation using the LabanEditor application.","title":"Introduction"},{"location":"GestureAuthoringTools/KinectReader/#tested-system-hardware-and-software","text":"Microsoft Windows 10, 64-bit Microsoft Kinect Sensor v2 for Windows Microsoft Kinect SDK v2.0 or newer for Kinect Sensor v2 (https://www.microsoft.com/en-us/download/details.aspx?id=44561) OpenCV Version 3.4.3 for Windows (https://sourceforge.net/projects/opencvlibrary/files/opencv-win/3.4.3/opencv-3.4.3-vc14_vc15.exe/download) Microsoft Visual C++ Runtime Library (https://aka.ms/vs/16/release/vc_redist.x64.exe)","title":"Tested System Hardware and Software"},{"location":"GestureAuthoringTools/KinectReader/#installation","text":"Download the entire KinectReader folder to your local machine - If needed, edit the file KinectReader.ini and set the DataDir property to a folder where the capture session data will be written. See INI File section below - Locate and place a copy of the OpenCV file opencv_world343.dll into the application folder, or add the existing location to your system's Path using Windows' Advanced System Properties control panel to set an Environment Variable .","title":"Installation"},{"location":"GestureAuthoringTools/KinectReader/#example-use","text":"The user wishes to create a set of reference gestures with associated audio and video. - Launch the KinectReader app. - From the drop-down control on the right, select Record Audio Video. - Click on Start Capture. - Perform each gesture and vocally name the gesture. - Click on Stop Capture. - The Kinect body data frames are saved to joints.csv and the audio/video recording is saved to videorecording.wmv.","title":"Example Use"},{"location":"GestureAuthoringTools/KinectReader/#starting-up-and-shutdown","text":"To run, double-click KinectReader.exe. To exit, click on the close button at the top of the window or press Alt+F4 and click Yes on the close confirmation message box.","title":"Starting Up and Shutdown"},{"location":"GestureAuthoringTools/KinectReader/#the-user-interface","text":"The main application windows has three main areas: the Kinect camera display, status, and controls.","title":"The User Interface"},{"location":"GestureAuthoringTools/KinectReader/#kinect-camera-display","text":"The Kinect Camera Display at the bottom displays the color image from the Kinect sensor overlaid with a diagram of the body tracking data for each person being tracked by the camera. Segments recognized by the Kinect sensor are green and inferred segments are blue. Segments that are not recognized are not shown. A red box highlights the person whose body tracking data will be recorded. If the wrong person is being tracked, click on the head of the correct person.","title":"Kinect Camera Display"},{"location":"GestureAuthoringTools/KinectReader/#status","text":"The Status area at the top-left displays messages such as a capture session is active and when a frame is captured.","title":"Status"},{"location":"GestureAuthoringTools/KinectReader/#controls","text":"The Controls area at the top-right configure and control a recording session. - The media drop-down control specifies whether to record audio video, audio-only, or not at all along with the Kinect data. - The Start Capture button begins a continuous capture session. - The Start Mark button begins a frame-by-frame capture session. A Kinect data frame is only captured when the Mark button is pressed. - The Stop Capture button stop the capture session and records the Kinect data, audio, and video to the output files. The video is recorded from the Kinect\u2019s color camera and the audio is recorded from the system default audio input device. Set the input audio device or adjust the audio input level using the Windows Sound settings or control panel.","title":"Controls"},{"location":"GestureAuthoringTools/KinectReader/#capture-sessions","text":"A continuous capture session records Kinect data frames, audio, and video as they are supplied by the Kinect sensor. A frame-by-frame capture session records a Kinect data frame only when the Mark button is clicked. When enabled, audio/video is recorded continuously for either session type. The session files are saved to the directory specified by the DataDir property: - The Kinect data frames are saved to joints.csv. - When recording both audio and video, the recording is saved to videorecording.wmv. - When recording audio only, the recording is saved to audiorecording.wma.","title":"Capture Sessions"},{"location":"GestureAuthoringTools/KinectReader/#reference","text":"","title":"Reference"},{"location":"GestureAuthoringTools/KinectReader/#keyboard-shortcuts","text":"Keys Description Alt+F4 Exit the application ## INI File [Section] Property=Value Type -- -- [main] DataDir= String ## Kinect Body Data File Format The Kinect body data file is a Comma-Separated Values (CSV) text file (*.csv). A data frame is a line of text terminated by a CR/LF pair and consists of a series of fields separated by commas (\u201c,\u201d). Each field is the C programming language printf()-style text representation of the value. The fields are the following: Field Type Description Timestamp Integer Kinect sensor timestamp in hundred-nanosecond units (10,000,000 units = 1 second.) The absolute value of the timestamp has no meaning but the timestamp allows calculating the elapsed time between frames and ordering frames by when they were captured. Joint Array Tracked Joint A series of Tracked Joint entries, one per each of 25 joints. The fields that make up the Joint Array immediately follow the Timestamp field. Each set of four fields form a Tracked Joint entry, one per each of 25 joints. There is no special delineation between entries in the array nor between each field in each entry beyond the standard comma. The Tracked Joint fields are: Field Type Description X Float X coordinate of the joint in Kinect camera space Y Float Y coordinate of the joint in Kinect camera space Z Float Z coordinate of the joint in Kinect camera space Tracking State Integer 0 = joint is not tracked, 1 = joint position is inferred (estimated), 2 = joint is tracked Kinect camera space is defined as follows: - The origin (X = 0, Y = 0, Z = 0) is at the center of the Kinect\u2019s IR sensor - X increases to the sensor\u2019s left, from the sensor looking at the joint - Y increases towards the sensor\u2019s top - Z increases away from the sensor to the joint - Each whole unit is one meter (1.0 units = 1.0 meters) See the Microsoft documentation Kinect for Windows SDK 2.0, \u201cProgramming Guide\u201d, \u201cCoordinate mapping\u201d for more details. The joints appear in the Joint Array in this order: 1. Spine Base 1. Spine Midsection 1. Neck 1. Head 1. Left Shoulder 1. Left Elbow 1. Left Wrist 1. Left Hand 1. Right Shoulder 1. Right Elbow 1. Right Wrist 1. Right Hand 1. Left Hip 1. Left Knee 1. Left Ankle 1. Left Foot 1. Right Hip 1. Right Knee 1. Right Ankle 1. Right Foot 1. Spine Shoulder 1. Left Hand Tip 1. Left Thumb 1. Right Hand Tip 1. Right Thumb","title":"Keyboard Shortcuts"},{"location":"GestureAuthoringTools/KinectReader/Data/","text":"","title":"Home"},{"location":"GestureAuthoringTools/KinectReader/Data/kinect_pictures/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics LabanEditor User Manual Introduction LabanEditor is a Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams presented in the user interface. Tested System Software Windows (Version 10, 64-bit) or Linux (Ubuntu18.04, 64-bit) Git 2.20.1 Python 2.7.10 (https://www.python.org/downloads/release/python-2710/) OpenCV 4.1.0 (https://sourceforge.net/projects/opencvlibrary/files/4.1.0/) MatPlotLib 2.2.4 (https://matplotlib.org/users/installing.html) NumPy 1.16.3 (https://github.com/numpy/numpy/releases/tag/v1.16.3) Tkinter 81008 (https://docs.python.org/3/library/tkinter.html) Installation Windows From a cmd.exe or other terminal shell: Create a folder for the installation in any convenient location and make it the current directory: mkdir (your desired folder name) cd (your desired folder path) Download and run the Python installer from this link Download and run the ActiveTcl library (Tkinter) installer from this link Download the Pip installer to your installation folder from this link Run these commands: python get-pip.py python -m pip install numpy python -m pip install matplotlib python -m pip install opencv-python Clone the repository: git clone --recursive https://github.com/MORL/LabanotationSuite.git Linux From a bash or other terminal shell: Create a folder for the installation in any convenient location and make it the current directory: $ mkdir (your desired folder name) $ cd (your desired folder path) Run these commands: $ sudo apt-get update $ sudo apt install python-pip $ python -m pip install numpy $ python -m pip install matplotlib $ python -m pip install opencv-python $ sudo apt-get install python-tk Clone the repository: $ git clone --recursive https://github.com/microsoft/LabanotationSuite.git Launch Application Windows From a cmd.exe or other terminal: Move to and then run the example batch file in the top level of the LabanEditor folder: cd LabanotationSuite\\GestureAuthoringTools\\LabanEditor run_main.bat The example batch file is provided for convenience and contains command line arguments to load the \"Total\" algorithm and the gesture file \"Ges01_wavehand.csv\" when the application launches. Alternatively, you can run a command directly such as: python src\\main.py --alg total --inputfile Ges01_wavehand.csv The application's four windows will appear on your screen. Note! If a Windows security warning appears preventing program execution, select \"more info\", and then select \"run anyway\". Linux From a bash or other terminal shell: Move to the src folder and then run a python command to run the file \"main.py\". The following example includes arguments to load the \"Total\" algorithm and the gesture file \"Ges01_wavehand.csv\" when the application launches: $ cd LabanotationSuite/GestureAuthoringTools/LabanEditor/src $ python main.py --alg total --inputfile Ges01_wavehand.csv Launch-time arguments: main.py [--alg]algorithm [--inputfile][path][filename] [--outputfile][path] [--nogui] --alg total , parallel or naive processing algorithm used to extract keyframes; if not specified, total is used --inputfile Required. Path and filename of .csv joint data file --outputfolder Destination folder for processed labanotation files in .json format; Destination folder for output .txt and .png graphic files; if not specified, output folder is .\\\\data_output --nogui For batch use; processes .csv joint file and extracts keyframes using specified algorithm but does not launch interactive GUI; extracts keyframes using specified algorithm The User Interface Gesture Analysis Configuration Screen This screen provides a user interface for: * Selecting a .csv input file for analysis * Selecting the algorithm used to extract keyframes from the .csv file * Generating .json and .txt gesture output files resulting from the application of the Gaussian Filter * Generating screen-grab graphic files of the application screens * Adjusting the Window Size and Sigma values for the Gaussian Filter and viewing a graph of its curve * Viewing the most recent lines of the application's Console Log KeyFrame Extraction Algorithms Note: Pre-Extraction Keyframe Extrapolation: Before keyframes are extracted, the application first performs a linear 3D interpolation to fill any missing samples in the original gesture .csv file. This interpolation process provides a first pass towards smoothing the movement of the gesture, but may generate imprecise results with some complicated gestures. The utility of this application is partly to allow a human user to review the gesture movement data and edit it if necessary. Alternatively, the --nogui launch-time argument allows this application to be used in a batch file to speed the processing of a large set of gestures. The Total energy algorithm extracts keyframes by calculating the total energy across all of the skeleton joints at each time sample and places the keyframes at signal peaks. The Parallel energy algorithm extracts keyframes by calculating the energy of each skeleton joint independently and in parallel at each time sample, averages the values and places the keyframes at signal valleys. The Naive (no-filter) algorithm does not apply an extraction algorithm and places a keyframe at each time sample. Output Files Clicking on the buttons in the Output Files area will create files in the folder \\data_output, unless another output folder was specified with a launch-time argument. The top two buttons will create data files containing containing labanotation keyframes in .json or .txt formats. The remaining buttons will create screen-grab graphic files of the three data views, as well as the full labanotation score in the .png format. Gaussian Filter Graph and Controls The graph shows the bell-curve of the Gaussian Filter with the selected sample window size and selected sigma value. The filter parameters can be adjusted by selecting new values on the bars under the graph and clicking the Apply button. The red vertical lines on the parameter selection bars indicate the default values applied when the application starts. Clicking the Reset button or changing between the total or parallel algorithms will return the filter parameters to the default values. Console Log The last three lines written to the terminal window that launched the application are visible in the Console Log. The entire application log for the session can be viewed in the terminal window itself. Filter Graph Viewer and KeyFrame Editor When using the Total or Parrallel keyframe extraction algorithms, this screen shows the graph of measured energy over time, as well as the position of the extracted keyframes. Total Energy Algorithm Filter Graph Screen Parallel Energy Algorithm Filter Graph Screen KeyFrame Editing The keyframe positions appear as green stars and can be manually edited with a mouse: * Right-click and drag on a key frame star to MOVE it along the graph line * Right-click on the graph line to CREATE a new keyframe star * Double-right-click on a keyframe star to DELETE it Naive (no-filter) Filter Graph Screen When using the Naive method to extract keyframes, there is no calculated energy curve to be represented. The color of the vertical bars simply indicate which keyframes along the gesture sample represent the originally captured joint position data and which frames were interpolated to fill any gaps in the sample set. 3D Joint Data and Time-line Viewer This screen shows 3D skeletons representing joint positions of a gesture over time in both the original input .csv file and what will be written to the output labanotation files. The top horizontal bar shows the original data from the .csv file with measured frames in blue and any interpolated frames in red. The middle horizontal bar shows the extracted keyframe positions as green vertical lines that can be written out as .json or .txt files. The bottom horizontal bar indicates time and the current position within the gesture, as well as provides the ability to animate the gesture by dragging the mouse across the bar. Labanotation Score Viewer This screen shows the gesture output in the .json file presented as a human-readable labanotation score.","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"GestureAuthoringTools/LabanEditor/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"GestureAuthoringTools/LabanEditor/#labaneditor-user-manual","text":"","title":"LabanEditor User Manual"},{"location":"GestureAuthoringTools/LabanEditor/#introduction","text":"LabanEditor is a Python script application that loads a Kinect joint .csv file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a .json file format suitable for controlling robots running a gesture interpretation driver, as well as .png graphic file renderings of the charts and diagrams presented in the user interface.","title":"Introduction"},{"location":"GestureAuthoringTools/LabanEditor/#tested-system-software","text":"Windows (Version 10, 64-bit) or Linux (Ubuntu18.04, 64-bit) Git 2.20.1 Python 2.7.10 (https://www.python.org/downloads/release/python-2710/) OpenCV 4.1.0 (https://sourceforge.net/projects/opencvlibrary/files/4.1.0/) MatPlotLib 2.2.4 (https://matplotlib.org/users/installing.html) NumPy 1.16.3 (https://github.com/numpy/numpy/releases/tag/v1.16.3) Tkinter 81008 (https://docs.python.org/3/library/tkinter.html)","title":"Tested System Software"},{"location":"GestureAuthoringTools/LabanEditor/#installation","text":"","title":"Installation"},{"location":"GestureAuthoringTools/LabanEditor/#windows","text":"","title":"Windows"},{"location":"GestureAuthoringTools/LabanEditor/#from-a-cmdexe-or-other-terminal-shell","text":"Create a folder for the installation in any convenient location and make it the current directory: mkdir (your desired folder name) cd (your desired folder path) Download and run the Python installer from this link Download and run the ActiveTcl library (Tkinter) installer from this link Download the Pip installer to your installation folder from this link Run these commands: python get-pip.py python -m pip install numpy python -m pip install matplotlib python -m pip install opencv-python Clone the repository: git clone --recursive https://github.com/MORL/LabanotationSuite.git","title":"From a cmd.exe or other terminal shell:"},{"location":"GestureAuthoringTools/LabanEditor/#linux","text":"","title":"Linux"},{"location":"GestureAuthoringTools/LabanEditor/#from-a-bash-or-other-terminal-shell","text":"Create a folder for the installation in any convenient location and make it the current directory: $ mkdir (your desired folder name) $ cd (your desired folder path) Run these commands: $ sudo apt-get update $ sudo apt install python-pip $ python -m pip install numpy $ python -m pip install matplotlib $ python -m pip install opencv-python $ sudo apt-get install python-tk Clone the repository: $ git clone --recursive https://github.com/microsoft/LabanotationSuite.git","title":"From a bash or other terminal shell:"},{"location":"GestureAuthoringTools/LabanEditor/#launch-application","text":"","title":"Launch Application"},{"location":"GestureAuthoringTools/LabanEditor/#windows_1","text":"","title":"Windows"},{"location":"GestureAuthoringTools/LabanEditor/#from-a-cmdexe-or-other-terminal","text":"Move to and then run the example batch file in the top level of the LabanEditor folder: cd LabanotationSuite\\GestureAuthoringTools\\LabanEditor run_main.bat The example batch file is provided for convenience and contains command line arguments to load the \"Total\" algorithm and the gesture file \"Ges01_wavehand.csv\" when the application launches. Alternatively, you can run a command directly such as: python src\\main.py --alg total --inputfile Ges01_wavehand.csv The application's four windows will appear on your screen. Note! If a Windows security warning appears preventing program execution, select \"more info\", and then select \"run anyway\".","title":"From a cmd.exe or other terminal:"},{"location":"GestureAuthoringTools/LabanEditor/#linux_1","text":"","title":"Linux"},{"location":"GestureAuthoringTools/LabanEditor/#from-a-bash-or-other-terminal-shell_1","text":"Move to the src folder and then run a python command to run the file \"main.py\". The following example includes arguments to load the \"Total\" algorithm and the gesture file \"Ges01_wavehand.csv\" when the application launches: $ cd LabanotationSuite/GestureAuthoringTools/LabanEditor/src $ python main.py --alg total --inputfile Ges01_wavehand.csv","title":"From a bash or other terminal shell:"},{"location":"GestureAuthoringTools/LabanEditor/#launch-time-arguments","text":"main.py [--alg]algorithm [--inputfile][path][filename] [--outputfile][path] [--nogui] --alg total , parallel or naive processing algorithm used to extract keyframes; if not specified, total is used --inputfile Required. Path and filename of .csv joint data file --outputfolder Destination folder for processed labanotation files in .json format; Destination folder for output .txt and .png graphic files; if not specified, output folder is .\\\\data_output --nogui For batch use; processes .csv joint file and extracts keyframes using specified algorithm but does not launch interactive GUI; extracts keyframes using specified algorithm","title":"Launch-time arguments:"},{"location":"GestureAuthoringTools/LabanEditor/#the-user-interface","text":"","title":"The User Interface"},{"location":"GestureAuthoringTools/LabanEditor/#gesture-analysis-configuration-screen","text":"This screen provides a user interface for: * Selecting a .csv input file for analysis * Selecting the algorithm used to extract keyframes from the .csv file * Generating .json and .txt gesture output files resulting from the application of the Gaussian Filter * Generating screen-grab graphic files of the application screens * Adjusting the Window Size and Sigma values for the Gaussian Filter and viewing a graph of its curve * Viewing the most recent lines of the application's Console Log","title":"Gesture Analysis Configuration Screen"},{"location":"GestureAuthoringTools/LabanEditor/#keyframe-extraction-algorithms","text":"Note: Pre-Extraction Keyframe Extrapolation: Before keyframes are extracted, the application first performs a linear 3D interpolation to fill any missing samples in the original gesture .csv file. This interpolation process provides a first pass towards smoothing the movement of the gesture, but may generate imprecise results with some complicated gestures. The utility of this application is partly to allow a human user to review the gesture movement data and edit it if necessary. Alternatively, the --nogui launch-time argument allows this application to be used in a batch file to speed the processing of a large set of gestures. The Total energy algorithm extracts keyframes by calculating the total energy across all of the skeleton joints at each time sample and places the keyframes at signal peaks. The Parallel energy algorithm extracts keyframes by calculating the energy of each skeleton joint independently and in parallel at each time sample, averages the values and places the keyframes at signal valleys. The Naive (no-filter) algorithm does not apply an extraction algorithm and places a keyframe at each time sample.","title":"KeyFrame Extraction Algorithms"},{"location":"GestureAuthoringTools/LabanEditor/#output-files","text":"Clicking on the buttons in the Output Files area will create files in the folder \\data_output, unless another output folder was specified with a launch-time argument. The top two buttons will create data files containing containing labanotation keyframes in .json or .txt formats. The remaining buttons will create screen-grab graphic files of the three data views, as well as the full labanotation score in the .png format.","title":"Output Files"},{"location":"GestureAuthoringTools/LabanEditor/#gaussian-filter-graph-and-controls","text":"The graph shows the bell-curve of the Gaussian Filter with the selected sample window size and selected sigma value. The filter parameters can be adjusted by selecting new values on the bars under the graph and clicking the Apply button. The red vertical lines on the parameter selection bars indicate the default values applied when the application starts. Clicking the Reset button or changing between the total or parallel algorithms will return the filter parameters to the default values.","title":"Gaussian Filter Graph and Controls"},{"location":"GestureAuthoringTools/LabanEditor/#console-log","text":"The last three lines written to the terminal window that launched the application are visible in the Console Log. The entire application log for the session can be viewed in the terminal window itself.","title":"Console Log"},{"location":"GestureAuthoringTools/LabanEditor/#filter-graph-viewer-and-keyframe-editor","text":"When using the Total or Parrallel keyframe extraction algorithms, this screen shows the graph of measured energy over time, as well as the position of the extracted keyframes.","title":"Filter Graph Viewer and KeyFrame Editor"},{"location":"GestureAuthoringTools/LabanEditor/#total-energy-algorithm-filter-graph-screen","text":"","title":"Total Energy Algorithm Filter Graph Screen"},{"location":"GestureAuthoringTools/LabanEditor/#parallel-energy-algorithm-filter-graph-screen","text":"","title":"Parallel Energy Algorithm Filter Graph Screen"},{"location":"GestureAuthoringTools/LabanEditor/#keyframe-editing","text":"The keyframe positions appear as green stars and can be manually edited with a mouse: * Right-click and drag on a key frame star to MOVE it along the graph line * Right-click on the graph line to CREATE a new keyframe star * Double-right-click on a keyframe star to DELETE it","title":"KeyFrame Editing"},{"location":"GestureAuthoringTools/LabanEditor/#naive-no-filter-filter-graph-screen","text":"When using the Naive method to extract keyframes, there is no calculated energy curve to be represented. The color of the vertical bars simply indicate which keyframes along the gesture sample represent the originally captured joint position data and which frames were interpolated to fill any gaps in the sample set.","title":"Naive (no-filter) Filter Graph Screen"},{"location":"GestureAuthoringTools/LabanEditor/#3d-joint-data-and-time-line-viewer","text":"This screen shows 3D skeletons representing joint positions of a gesture over time in both the original input .csv file and what will be written to the output labanotation files. The top horizontal bar shows the original data from the .csv file with measured frames in blue and any interpolated frames in red. The middle horizontal bar shows the extracted keyframe positions as green vertical lines that can be written out as .json or .txt files. The bottom horizontal bar indicates time and the current position within the gesture, as well as provides the ability to animate the gesture by dragging the mouse across the bar.","title":"3D Joint Data and Time-line Viewer"},{"location":"GestureAuthoringTools/LabanEditor/#labanotation-score-viewer","text":"This screen shows the gesture output in the .json file presented as a human-readable labanotation score.","title":"Labanotation Score Viewer"},{"location":"GestureAuthoringTools/LabanEditor/data_input/","text":"Use this folder for input .csv gesture files. For batch use, input file can be defined at launch-time by using the command-one argument: \"--inputfile [file path]\".","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/data_output/","text":"By default, .json, .txt, and graphic files will be saved in this folder. For batch use, file name and path can be defined on launch by using the command-one argument: \"--outputfolder [file path]\". For this use, destination folder must already exist.","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/","text":"Source code for the LabanEditor application. ToDo: Folder and file descriptions","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/graphFilter/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/graphLaban/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/graphSkeleton/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/guiMenu/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/labanotation/","text":"","title":"Home"},{"location":"GestureAuthoringTools/LabanEditor/src/labanotation/tool/","text":"","title":"Home"},{"location":"MSRAbotSimulation/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics MSRAbot Simulation Software Description The MSRAbot Simulation Software uses javascript and html code to implement an animated 3D model of the robot and a user interface for selecting and rendering gestures described in the JSON format. A temporary local HTTP server invoked with python or an existing server can be used to host the software and the simulation is run within a modern web browser. The user can choose from a collection of sample gestures, or select a new gesture captured and created using this project's Gesture Authoring Tools. Tested System Software Microsoft Windows 10, 64-bit Microsoft Edge Web Browser (https://www.microsoft.com/en-us/edge/) Python 2.7.10 (https://www.python.org/downloads/release/python-2710/) Installation Copy the entire MSRAbotSimulation folder into a convenient folder on your local computer. Note: If you are already running or have access to an HTTP server, you can copy this folder into that server's file tree. Starting Up Open a command prompt or terminal session with access to your python installation Navigate to the MSRAbotSimulation folder containing the file index.html Run the following command (for python 2.3): python -m SimpleHTTPServer Note: If you are running python version 3 or higher, run this command: python3 -m http.server Open your favorite browser and open the URL http://localhost:8000 Note: If you have installed the folder into an existing HTTP server, you do not need to run the previous python commands but will need to adjust the URL for that particular server. For example: http://[your_server_name/your_installation_path]/MSRAbotSimulation/index.html Viewing Gestures Animated gestures are viewed in the browser by selecting and loading JSON files. View angle, panning, and zoom are available by clicking and holding mouse buttons and scroll wheels within the scene. Animation speed can be adjusted with a slider bar in the upper-right control panel. Select a Gesture Changing the gesture to be viewed is performed by selecting the drop-down text box in the upper-right corner of the browser screen. A selection of sample gestures are available, as well as an ability to load a newly created gesture stored on the local computer. In addition to the six gestures, the sample files contain examples of the three keyframe extraction methods (naive, total, and parallel) available for processing in the LabanEditor gesture authoring tool included in this suite. Select a Local Gesture File New gesture files created by the user can be viewed by selecting Import/Upload JSON File in the drop-down text box and then navigating to the file on the local computer: staticElbow The fixed elbow joint of the MSRAbot physical robot can be modeled or ignored by checking the staticElbow box. This enforces a movement constraint to the model that matches a physical MSRAbot device. A humanoid stick figure model does not have this constraint. Note: If the following showHelpers checkbox is also set, the unconstrained humanoid skeletal model may follow different paths than the MSRAbot model. showHelpers The skeletal components of a humanoid stick figure model can be viewed by checking the showHelpers box and adjusting the opacity of the MSRAbot model.","title":"Home"},{"location":"MSRAbotSimulation/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"MSRAbotSimulation/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"MSRAbotSimulation/#msrabot-simulation-software","text":"","title":"MSRAbot Simulation Software"},{"location":"MSRAbotSimulation/#description","text":"The MSRAbot Simulation Software uses javascript and html code to implement an animated 3D model of the robot and a user interface for selecting and rendering gestures described in the JSON format. A temporary local HTTP server invoked with python or an existing server can be used to host the software and the simulation is run within a modern web browser. The user can choose from a collection of sample gestures, or select a new gesture captured and created using this project's Gesture Authoring Tools.","title":"Description"},{"location":"MSRAbotSimulation/#tested-system-software","text":"Microsoft Windows 10, 64-bit Microsoft Edge Web Browser (https://www.microsoft.com/en-us/edge/) Python 2.7.10 (https://www.python.org/downloads/release/python-2710/)","title":"Tested System Software"},{"location":"MSRAbotSimulation/#installation","text":"Copy the entire MSRAbotSimulation folder into a convenient folder on your local computer. Note: If you are already running or have access to an HTTP server, you can copy this folder into that server's file tree.","title":"Installation"},{"location":"MSRAbotSimulation/#starting-up","text":"Open a command prompt or terminal session with access to your python installation Navigate to the MSRAbotSimulation folder containing the file index.html Run the following command (for python 2.3): python -m SimpleHTTPServer Note: If you are running python version 3 or higher, run this command: python3 -m http.server Open your favorite browser and open the URL http://localhost:8000 Note: If you have installed the folder into an existing HTTP server, you do not need to run the previous python commands but will need to adjust the URL for that particular server. For example: http://[your_server_name/your_installation_path]/MSRAbotSimulation/index.html","title":"Starting Up"},{"location":"MSRAbotSimulation/#viewing-gestures","text":"Animated gestures are viewed in the browser by selecting and loading JSON files. View angle, panning, and zoom are available by clicking and holding mouse buttons and scroll wheels within the scene. Animation speed can be adjusted with a slider bar in the upper-right control panel.","title":"Viewing Gestures"},{"location":"MSRAbotSimulation/#select-a-gesture","text":"Changing the gesture to be viewed is performed by selecting the drop-down text box in the upper-right corner of the browser screen. A selection of sample gestures are available, as well as an ability to load a newly created gesture stored on the local computer. In addition to the six gestures, the sample files contain examples of the three keyframe extraction methods (naive, total, and parallel) available for processing in the LabanEditor gesture authoring tool included in this suite.","title":"Select a Gesture"},{"location":"MSRAbotSimulation/#select-a-local-gesture-file","text":"New gesture files created by the user can be viewed by selecting Import/Upload JSON File in the drop-down text box and then navigating to the file on the local computer:","title":"Select a Local Gesture File"},{"location":"MSRAbotSimulation/#staticelbow","text":"The fixed elbow joint of the MSRAbot physical robot can be modeled or ignored by checking the staticElbow box. This enforces a movement constraint to the model that matches a physical MSRAbot device. A humanoid stick figure model does not have this constraint. Note: If the following showHelpers checkbox is also set, the unconstrained humanoid skeletal model may follow different paths than the MSRAbot model.","title":"staticElbow"},{"location":"MSRAbotSimulation/#showhelpers","text":"The skeletal components of a humanoid stick figure model can be viewed by checking the showHelpers box and adjusting the opacity of the MSRAbot model.","title":"showHelpers"}]}